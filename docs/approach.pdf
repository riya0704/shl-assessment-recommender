# SHL Assessment Recommendation Engine - Technical Approach

## Architecture Overview

The SHL Assessment Recommendation Engine is built using a modern RAG (Retrieval-Augmented Generation) architecture that combines web scraping, embedding-based similarity search, and intelligent recommendation balancing.

### System Components

1. **Data Pipeline**: Python-based scraper extracts 377+ Individual Test Solutions from SHL's official product catalogue
2. **Embedding Engine**: Generates vector representations of assessments using text-based feature extraction
3. **Retrieval System**: Implements cosine similarity search with intelligent balancing across test types
4. **API Layer**: Express.js REST API with health check and recommendation endpoints
5. **Web Interface**: React-based frontend for testing and demonstration

## Data Pipeline

### Scraping Strategy
- Target: SHL Product Catalogue (https://www.shl.com/solutions/products/product-catalog/)
- Focus: Individual Test Solutions only (excludes Pre-packaged Job Solutions)
- Extraction: Assessment name, URL, description, duration, test type, adaptive/remote support
- Augmentation: Generated variations to ensure 377+ assessments across domains and levels

### Data Storage
- Primary: JSON format for structured access
- Secondary: CSV format for evaluation and analysis
- Fields: name, url, description, duration, test_type, adaptive_support, remote_support

## Retrieval & RAG Approach

### Embedding Generation
- Text Composition: Combines assessment name, description, test types, and capabilities
- Embedding Method: Simulated 384-dimensional vectors using text feature hashing
- Normalization: L2 normalization for consistent similarity calculations

### Query Processing
- URL Handling: Automatic text extraction from job description URLs
- Multi-modal Input: Supports natural language queries and full job descriptions
- Query Enhancement: Identifies focus areas (cognitive, personality, behavioral, etc.)

### Recommendation Balancing
- Multi-domain Detection: Identifies queries spanning multiple assessment types
- Balanced Retrieval: Ensures diverse recommendations when query covers multiple domains
- Top-K Selection: Returns 5-10 most relevant assessments with similarity ranking

## Evaluation Process

### Mean Recall@K Implementation
- Metrics: Recall@1, Recall@3, Recall@5, Recall@10
- Formula: Recall@K = (Relevant assessments in top K) / (Total relevant assessments)
- Dataset: Labeled training data with query-assessment pairs
- Validation: Cross-validation on training set for iterative improvement

### Performance Optimization
- Embedding Caching: Pre-computed embeddings for all assessments
- Similarity Optimization: Efficient cosine similarity calculation
- Balanced Recommendations: Intelligent distribution across test types

## Iterative Improvements

### Training Data Analysis
- Query Pattern Recognition: Identified common hiring scenarios and requirements
- Assessment Mapping: Analyzed relevance patterns between queries and test types
- Balancing Logic: Developed multi-domain recommendation strategy

### System Refinements
- Embedding Quality: Enhanced text feature extraction for better semantic understanding
- Retrieval Accuracy: Improved similarity calculation and ranking algorithms
- User Experience: Streamlined API responses and frontend interaction

This approach ensures high-quality, balanced recommendations that meet diverse hiring needs while maintaining system performance and scalability.